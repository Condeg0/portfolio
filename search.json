[
  {
    "objectID": "Uber-NYC-Analysis/index.html",
    "href": "Uber-NYC-Analysis/index.html",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "",
    "text": "This project analyzes 4.5 million Uber pickups in New York City to identify high-demand locations and time windows. Using R (ggplot2, dplyr), I processed trip logs to answer three operational questions: 1. Where are pickups concentrated? 2. When does demand peak during the day? 3. How does fleet activity shift between weekdays and weekends?"
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#project-overview",
    "href": "Uber-NYC-Analysis/index.html#project-overview",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "",
    "text": "This project analyzes 4.5 million Uber pickups in New York City to identify high-demand locations and time windows. Using R (ggplot2, dplyr), I processed trip logs to answer three operational questions: 1. Where are pickups concentrated? 2. When does demand peak during the day? 3. How does fleet activity shift between weekdays and weekends?"
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#geographic-distribution",
    "href": "Uber-NYC-Analysis/index.html#geographic-distribution",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "1. Geographic Distribution",
    "text": "1. Geographic Distribution\nQuestion: Is demand spread evenly across the city?\n\nFinding: The pickup density is heavily skewed.\n\nManhattan Core: The vast majority of rides occur in Manhattan (below 59th St) and Downtown Brooklyn.\nAirport Clusters: Outside the city grid, the only significant hotspots are JFK and LaGuardia Airports.\nOperational Insight: For a driver, leaving Manhattan without an airport fare is statistically inefficient, as return trips from outer boroughs are rare."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#hourly-demand-patterns",
    "href": "Uber-NYC-Analysis/index.html#hourly-demand-patterns",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "2. Hourly Demand Patterns",
    "text": "2. Hourly Demand Patterns\nQuestion: Is there a consistent “best time” to drive?\n\nFinding:Demand follows a strict daily cycle that is consistent across all six months (Apr–Sep).\n\nThe Build-Up: Ride volume begins to rise significantly at 16:00 (4 PM).\nThe Peak: The busiest hour is consistently 17:00–18:00 (5–6 PM), coinciding with the end of the traditional workday.\nLate Night: Demand tapers off after 21:00, except on weekends (see below)."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#heatmap-analysis-day-vs.-hour",
    "href": "Uber-NYC-Analysis/index.html#heatmap-analysis-day-vs.-hour",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "3. Heatmap Analysis: Day vs. Hour",
    "text": "3. Heatmap Analysis: Day vs. Hour\nQuestion: How does the schedule change from Tuesday to Saturday?\n\nFinding: Aggregating data by day of the week reveals two distinct distinct “shifts”:\n\nThe Commuter Shift (Mon–Thu): Demand is strictly focused on rush hours (07:00–09:00 and 17:00–19:00).\nThe Nightlife Shift (Fri–Sat): The evening peak extends well past midnight, staying high until 02:00 AM.\n\nBusiness Implication: Algorithms should prioritize “commuter” routes during the week but switch to “entertainment district” positioning on Friday nights."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#base-growth-fleet-scaling",
    "href": "Uber-NYC-Analysis/index.html#base-growth-fleet-scaling",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "4. Base Growth (Fleet Scaling)",
    "text": "4. Base Growth (Fleet Scaling)\nQuestion: Are all Uber bases growing at the same rate?\n\nFinding: While most bases (like B02598 and B02682) showed stable monthly volume, Base B02617 (Teal) grew exponentially.\n\nApril: ~30k trips\nSeptember: ~180k trips\nThis suggests that Uber likely consolidated drivers into this specific base or ran an aggressive onboarding campaign during the summer of 2014."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#technical-implementation",
    "href": "Uber-NYC-Analysis/index.html#technical-implementation",
    "title": "Uber NYC Demand Analysis (Apr–Sep 2014)",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nStack: R, RStudio Key Libraries:\n\nggplot2: Used for the heatmaps and bar charts.\nlubridate: Parsed the timestamps to extract Hour, Day, and Month factors.\ndplyr: Grouped the 4.5M rows for aggregation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an aspiring Data Scientist and Quantitative Analyst with a rigorous academic background in both economic theory and data science. My objective is to leverage statistical modeling and machine learning to drive actionable insights in business.\nI am currently finishing my dual degree at the University of South Florida (May 2026) with a 3.89 GPA."
  },
  {
    "objectID": "about.html#professional-profile",
    "href": "about.html#professional-profile",
    "title": "About Me",
    "section": "",
    "text": "I am an aspiring Data Scientist and Quantitative Analyst with a rigorous academic background in both economic theory and data science. My objective is to leverage statistical modeling and machine learning to drive actionable insights in business.\nI am currently finishing my dual degree at the University of South Florida (May 2026) with a 3.89 GPA."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About Me",
    "section": "Research Interests",
    "text": "Research Interests\nMy academic and project work is driven by a focus on market behavior and risk:\n\nQuantitative Finance: Application of Monte Carlo simulations and stochastic calculus to option pricing.\nRisk Management: Interest in tail-risk management and non-Gaussian market distributions (referencing frameworks by N. Taleb and O. Peters).\nMachine Learning: Utilizing LSTMs and Random Forests for time-series forecasting and classification."
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\n\nDomain\nStack\n\n\n\n\nProgramming\nPython (Pandas, NumPy), R, SQL\n\n\nData Science\nPyTorch, Scikit-Learn, TensorFlow, Statsmodels\n\n\nMethods\nRandom Forests, Linear Regressions, Decision Trees, Neural Networks, ARIMA, ARMA, VAR,Monte Carlo Methods\n\n\nTools/Dev\nGit, Unix/Linux, Quarto"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\n\nCompetitive Athletics: Former top-ranked junior tennis athlete (Brazil). This background has fostered a disciplined, goal-oriented approach to my professional work.\nLanguages: Portuguese (Native), English (Fluent), Spanish (Conversational)."
  },
  {
    "objectID": "Loan-Default-Prediction/index.html",
    "href": "Loan-Default-Prediction/index.html",
    "title": "Optimizing Credit Risk: A Profit-First Approach",
    "section": "",
    "text": "Executive Summary\nIn consumer lending, accuracy is a misleading metric. A model that blindly predicts “No Default” can achieve 85% accuracy but bankrupt a lender by missing the few massive defaults.\nThis project re-engineers the credit scoring process by shifting the optimization objective from Statistical Accuracy (F1-Score) to Economic Profit.\nBy developing a modular classification engine and applying a custom profit-maximization threshold, this strategy achieved:\n\nFinancial Impact: Projected portfolio value of $1.26M, a ~40% ROI lift over the baseline.\nRisk Policy: Identified an optimal cutoff rejecting the riskiest 17.3% of applicants.\nModel Performance: Achieved a 0.685 ROC-AUC using a defensible, regulator-friendly Logistic Regression model.\n\n\n\n\n1. The Business Problem\n\nAsymmetry of Errors\nIn credit risk, confusion matrix errors are not created equal:\n\nFalse Positive (Type I Error): We reject a good applicant. Cost = Opportunity Cost of Interest.\nFalse Negative (Type II Error): We approve a defaulter. Cost = Loss of Principal.\n\nSince the Principal is significantly larger than the Interest, avoiding False Negatives is mathematically more critical than avoiding False Positives. Standard “out-of-the-box” machine learning models often fail to capture this asymmetry by themselves.\n\n\nThe Objective\nBuild a system that answers: “What is the exact probability threshold that maximizes Net Portfolio Profit?”\n\n\n\n\n2. System Architecture\nTo ensure reproducibility and prevent data leakage, I moved beyond ad-hoc notebooks to a production-grade modular architecture.\n\nDirectory Structure\nThe project is organized as a Python package (src), separating business logic from training pipelines.\n├── src/\n│   ├── data_pipeline.py  # Custom Transformers (Financial Engineering)\n│   ├── training.py       # Cross-Validation & Hyperparameter Tuning\n│   ├── evaluation.py     # Profit & Strategy Curve Logic\n│   └── config.py         # Centralized Assumptions\n├── notebooks/            # Narrative Analysis\n└── README.md\n\n\nFeature Engineering Pipeline\nA critical component was the FinancialFeatureEngineer transformer, which derives economic variables before the model sees the data. This calculates the Present Value (PV) of the loan using standard amortization formulas.\n# src/data_pipeline.py Snippet\n\ndef engineer_financial_features(df):\n    \"\"\"\n    Derives the Principal (PV) using the Annuity Formula.\n    PV = Installment * (1 - (1+r)^-n) / r\n    \"\"\"\n    monthly_rate = df['int.rate'] / 12\n    term = 36 # Assumed 36-month term\n    \n    # Vectorized Calculation of Risk Exposure\n    df['principal'] = df['installment'] * (\n        1 - (1 + monthly_rate)**(-term)\n    ) / monthly_rate\n    \n    return df\n\n\n\n\n3. Modeling Strategy\n\nModel Selection\nI benchmarked three architectures:\n\nLogistic Regression: Linear, highly interpretable (industry standard for compliance).\nRandom Forest: Handles non-linearities but prone to overfitting on noise.\nXGBoost: Gradient boosting for maximizing weak signals.\n\nWinner: Logistic Regression. Surprisingly, the simpler linear model outperformed the complex tree ensembles on the test set (AUC 0.685 vs 0.67 & 0.66 respectively). This suggests the signal in this specific dataset is largely linear (e.g., FICO scores linearly correlate with risk), and the complex models were overfitting the noise.\n\n\nThe “Strategy Curve”\nInstead of accepting the default decision threshold of 0.5, I implemented a custom evaluation function to plot Profit vs. Approval Rate.\n\nX-Axis: Percentage of loans approved (sorted by safety).\nY-Axis: Total Portfolio Profit.\n\n This visualization allows stakeholders to visually “pick” their risk appetite.\n\n\n\n\n4. Results & Analysis\n\nPerformance Metrics\nThe final model demonstrated robust separation power between “Good” and “Bad” loans.\n\nROC-AUC: 0.685\nPrecision (Class 0 - Paid): 0.86\nRecall (Class 1 - Default): Tunable via threshold.\n\n\n\nBusiness Insight: The Optimization Peak\nThe analysis revealed a convex profit curve.\n\nAt 100% Approval: The portfolio absorbs all toxic assets, significantly reducing net profit.\nAt 82.7% Approval (The Peak): By cutting the bottom 17.3% of risky applicants, we maximize profit at $1,264,043.\nBelow 80% Approval: We start rejecting too many good customers, and the opportunity cost (lost interest) begins to outweigh the savings from avoided defaults.\n\n\n\n\n\n\n\nManagerial Conclusion: The optimal policy is strict but not draconian. We should approve the safest ~83% of applicants. This creates a projected 40% increase in profitability compared to a naive strategy.\n\n\n\n\n\n\n\n5. Key Risk Drivers (Interpretation)\nUsing coefficient analysis (and SHAP values for tree benchmarks), the primary drivers of default were:\n\nFICO Score: The single strongest predictor. Lower scores exponentially increase default odds.\nInquiries (Last 6 Months): A high velocity of recent credit applications is a strong distress signal.\nInterest Rate: The market is efficient—loans priced with higher rates did default more often, confirming that the original underwriters correctly identified risk (though they often underpriced it).\n\n\n\n\nConclusion\nThis project demonstrates that Data Science is not just about prediction; it is about decision-making.\nBy translating a classification problem into a financial optimization problem, I delivered a model that speaks the language of the business: Profit, ROI, and Risk Exposure.\n\nCode Repository: View on GitHub\nTech Stack: Python, Scikit-Learn, Pandas, Matplotlib."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rafael Condé Gomes",
    "section": "",
    "text": "I am a double major in Econometrics and Information Science (concentration in Data Science & Analytics) at the University of South Florida (Class of 2026)\nI believe valid decisions require solid evidence, yet they are rarely based on it.\nMy focus is bridging that gap. I combine econometric rigor with data science to transform ambiguity into actionable insights. My goal is to replace intuition with inference, helping organizations optimize efficiency and solve complex real-world problems with precision.\n\n\n\nTechnology Director | Hexa Junior Consulting Company\nResearch Associate | USF Student Investment Club (Macroeconomics)\nTutor | University of South Florida Athletics\n\n\n\n\n\nCredit Risk Optimization: Developed a profit-maximizing threshold model for loan default prediction.\n\n\n\n\n\n\n\nPortfolio & Code: View my technical projects here."
  },
  {
    "objectID": "index.html#data-scientist-quantitative-analyst",
    "href": "index.html#data-scientist-quantitative-analyst",
    "title": "Rafael Condé Gomes",
    "section": "",
    "text": "I am a double major in Econometrics and Information Science (concentration in Data Science & Analytics) at the University of South Florida (Class of 2026)\nI believe valid decisions require solid evidence, yet they are rarely based on it.\nMy focus is bridging that gap. I combine econometric rigor with data science to transform ambiguity into actionable insights. My goal is to replace intuition with inference, helping organizations optimize efficiency and solve complex real-world problems with precision.\n\n\n\nTechnology Director | Hexa Junior Consulting Company\nResearch Associate | USF Student Investment Club (Macroeconomics)\nTutor | University of South Florida Athletics\n\n\n\n\n\nCredit Risk Optimization: Developed a profit-maximizing threshold model for loan default prediction.\n\n\n\n\n\n\n\nPortfolio & Code: View my technical projects here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Technical Projects",
    "section": "",
    "text": "Stack: Python, Scikit-Learn, Pandas, Matplotlib, Seaborn\nThe Problem: Standard default prediction models maximize accuracy (AUC) but often fail to maximize profit. A bank loses more money on a False Negative (default) than it gains from a True Negative (repayment).\nThe Solution: I engineered a Threshold Optimization Engine that shifts the decision boundary based on the specific cost-benefit matrix of the loan portfolio.\n\nStrategy: Prioritized recall for high-risk segments to minimize costly defaults.\nResult: While raw accuracy remained comparable to baseline, the risk-adjusted revenue improved significantly by rejecting “borderline” applicants that standard models would have accepted.\n\nView Full Analysis & Code →\n\n\n\n\nStack: R (ggplot2, dplyr), Geospatial Analysis\nThe Problem: Ride-sharing platforms face a “Cold Start” problem every day: Where should drivers position themselves before demand spikes? Inefficient positioning leads to higher wait times and lost revenue.\nThe Solution: I conducted a spatiotemporal analysis of 4.5M+ Uber pickups to identify high-density clusters and temporal demand cycles.\n\nInsight: Identified a non-linear demand surge on Thursdays (not Fridays) suggesting commuter-heavy usage over nightlife.\nApplication: These heatmaps serve as a proxy for “Pre-Surge” indicators for fleet management.\n\nView Visual Analysis →"
  },
  {
    "objectID": "projects.html#featured-case-studies",
    "href": "projects.html#featured-case-studies",
    "title": "Technical Projects",
    "section": "",
    "text": "Stack: Python, Scikit-Learn, Pandas, Matplotlib, Seaborn\nThe Problem: Standard default prediction models maximize accuracy (AUC) but often fail to maximize profit. A bank loses more money on a False Negative (default) than it gains from a True Negative (repayment).\nThe Solution: I engineered a Threshold Optimization Engine that shifts the decision boundary based on the specific cost-benefit matrix of the loan portfolio.\n\nStrategy: Prioritized recall for high-risk segments to minimize costly defaults.\nResult: While raw accuracy remained comparable to baseline, the risk-adjusted revenue improved significantly by rejecting “borderline” applicants that standard models would have accepted.\n\nView Full Analysis & Code →\n\n\n\n\nStack: R (ggplot2, dplyr), Geospatial Analysis\nThe Problem: Ride-sharing platforms face a “Cold Start” problem every day: Where should drivers position themselves before demand spikes? Inefficient positioning leads to higher wait times and lost revenue.\nThe Solution: I conducted a spatiotemporal analysis of 4.5M+ Uber pickups to identify high-density clusters and temporal demand cycles.\n\nInsight: Identified a non-linear demand surge on Thursdays (not Fridays) suggesting commuter-heavy usage over nightlife.\nApplication: These heatmaps serve as a proxy for “Pre-Surge” indicators for fleet management.\n\nView Visual Analysis →"
  }
]