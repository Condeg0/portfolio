[
  {
    "objectID": "Uber-NYC-Analysis/index.html",
    "href": "Uber-NYC-Analysis/index.html",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "",
    "text": "Efficient fleet management requires predicting where demand will materialize before it happens. Using a dataset of 4.5 million Uber pickups in New York City (Apr–Sep 2014), this project isolates the temporal and geospatial patterns that drive urban mobility.\nThe Objective: Transform raw trip logs into actionable scheduling intelligence for driver allocation."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#executive-summary",
    "href": "Uber-NYC-Analysis/index.html#executive-summary",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "",
    "text": "Efficient fleet management requires predicting where demand will materialize before it happens. Using a dataset of 4.5 million Uber pickups in New York City (Apr–Sep 2014), this project isolates the temporal and geospatial patterns that drive urban mobility.\nThe Objective: Transform raw trip logs into actionable scheduling intelligence for driver allocation."
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#temporal-demand-cycles",
    "href": "Uber-NYC-Analysis/index.html#temporal-demand-cycles",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "1. Temporal Demand Cycles",
    "text": "1. Temporal Demand Cycles\nUnderstanding the “heartbeat” of the city is crucial for shift scheduling.\n\nHourly Drift\nDemand does not follow a bell curve. It follows a commuter-agglomeration pattern. The data reveals a slow buildup starting at 16:00, peaking at 17:00-18:00 (End of Business Day), and maintaining high liquidity until late evening.\n\n\n\n\n\n\nTip\n\n\n\nOperational Insight: Drivers logging off at 5 PM miss the highest volume window. Shift incentives should be weighted to capture the 17:00–21:00 block.\n\n\n(Insert your “Trips by Hour” bar chart here)\n\n\nThe “Thursday Phenomenon”\nContrary to the assumption that Friday/Saturday are the busiest days, the data shows Thursday consistently outperforming or matching Friday in trip volume.\n(Insert your “Trips by Day of Week” chart here)"
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#heatmap-analysis",
    "href": "Uber-NYC-Analysis/index.html#heatmap-analysis",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "2. Heatmap Analysis",
    "text": "2. Heatmap Analysis\nGranularity matters. Aggregated totals hide specific liquidity pockets. By mapping Hour vs. Day, we see the “dead zones” (early week mornings) vs. the “hot zones.”\n(Insert your “Heat Map by Hour and Day” chart here)"
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#geospatial-clustering",
    "href": "Uber-NYC-Analysis/index.html#geospatial-clustering",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "3. Geospatial Clustering",
    "text": "3. Geospatial Clustering\nThe most significant operational metric is location. The visualization below maps individual pickups. The density (visualized in blue) effectively recreates the map of Manhattan, showing that demand is strictly confined to the borough’s grid, with sparse outlier activity in the outer boroughs.\n(Insert your Blue NYC Map chart here)"
  },
  {
    "objectID": "Uber-NYC-Analysis/index.html#technical-implementation",
    "href": "Uber-NYC-Analysis/index.html#technical-implementation",
    "title": "NYC Fleet Allocation & Demand Analysis",
    "section": "Technical Implementation",
    "text": "Technical Implementation\nThe analysis was performed in R using ggplot2 for visualization and dplyr for high-performance data manipulation.\n```r # Sample of the Geospatial Plotting Logic min_lat &lt;- 40.5774 max_lat &lt;- 40.9176 min_long &lt;- -74.15 max_long &lt;- -73.7004\nggplot(data = data_2014, aes(x = Lon, y = Lat)) + geom_point(size = 1, color = “blue”) + scale_x_continuous(limits = c(min_long, max_long)) + scale_y_continuous(limits = c(min_lat, max_lat)) + theme_map() + ggtitle(“NYC Geospatial Demand Density”)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am an aspiring Data Scientist and Quantitative Analyst with a rigorous academic background in both economic theory and data science. My objective is to leverage statistical modeling and machine learning to drive actionable insights in business.\nI am currently finishing my dual degree at the University of South Florida (May 2026) with a 3.89 GPA."
  },
  {
    "objectID": "about.html#professional-profile",
    "href": "about.html#professional-profile",
    "title": "About Me",
    "section": "",
    "text": "I am an aspiring Data Scientist and Quantitative Analyst with a rigorous academic background in both economic theory and data science. My objective is to leverage statistical modeling and machine learning to drive actionable insights in business.\nI am currently finishing my dual degree at the University of South Florida (May 2026) with a 3.89 GPA."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About Me",
    "section": "Research Interests",
    "text": "Research Interests\nMy academic and project work is driven by a focus on market behavior and risk:\n\nQuantitative Finance: Application of Monte Carlo simulations and stochastic calculus to option pricing.\nRisk Management: Interest in tail-risk management and non-Gaussian market distributions (referencing frameworks by N. Taleb and O. Peters).\nMachine Learning: Utilizing LSTMs and Random Forests for time-series forecasting and classification."
  },
  {
    "objectID": "about.html#technical-skills",
    "href": "about.html#technical-skills",
    "title": "About Me",
    "section": "Technical Skills",
    "text": "Technical Skills\n\n\n\n\n\n\n\nDomain\nStack\n\n\n\n\nProgramming\nPython (Pandas, NumPy), R, SQL\n\n\nData Science\nPyTorch, Scikit-Learn, TensorFlow, Statsmodels\n\n\nMethods\nRandom Forests, Linear Regressions, Decision Trees, Neural Networks, ARIMA, ARMA, VAR,Monte Carlo Methods\n\n\nTools/Dev\nGit, Unix/Linux, Quarto"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "Background",
    "text": "Background\n\nCompetitive Athletics: Former top-ranked junior tennis athlete (Brazil). This background has fostered a disciplined, goal-oriented approach to my professional work.\nLanguages: Portuguese (Native), English (Fluent), Spanish (Conversational)."
  },
  {
    "objectID": "Loan-Default-Prediction/index.html",
    "href": "Loan-Default-Prediction/index.html",
    "title": "Optimizing Credit Risk: A Profit-First Approach",
    "section": "",
    "text": "Executive Summary\nIn consumer lending, accuracy is a misleading metric. A model that blindly predicts “No Default” can achieve 85% accuracy but bankrupt a lender by missing the few massive defaults.\nThis project re-engineers the credit scoring process by shifting the optimization objective from Statistical Accuracy (F1-Score) to Economic Profit.\nBy developing a modular classification engine and applying a custom profit-maximization threshold, this strategy achieved:\n\nFinancial Impact: Projected portfolio value of $1.26M, a ~40% ROI lift over the baseline.\nRisk Policy: Identified an optimal cutoff rejecting the riskiest 17.3% of applicants.\nModel Performance: Achieved a 0.685 ROC-AUC using a defensible, regulator-friendly Logistic Regression model.\n\n\n\n\n1. The Business Problem\n\nAsymmetry of Errors\nIn credit risk, confusion matrix errors are not created equal:\n\nFalse Positive (Type I Error): We reject a good applicant. Cost = Opportunity Cost of Interest.\nFalse Negative (Type II Error): We approve a defaulter. Cost = Loss of Principal.\n\nSince the Principal is significantly larger than the Interest, avoiding False Negatives is mathematically more critical than avoiding False Positives. Standard “out-of-the-box” machine learning models often fail to capture this asymmetry by themselves.\n\n\nThe Objective\nBuild a system that answers: “What is the exact probability threshold that maximizes Net Portfolio Profit?”\n\n\n\n\n2. System Architecture\nTo ensure reproducibility and prevent data leakage, I moved beyond ad-hoc notebooks to a production-grade modular architecture.\n\nDirectory Structure\nThe project is organized as a Python package (src), separating business logic from training pipelines.\n├── src/\n│   ├── data_pipeline.py  # Custom Transformers (Financial Engineering)\n│   ├── training.py       # Cross-Validation & Hyperparameter Tuning\n│   ├── evaluation.py     # Profit & Strategy Curve Logic\n│   └── config.py         # Centralized Assumptions\n├── notebooks/            # Narrative Analysis\n└── README.md\n\n\nFeature Engineering Pipeline\nA critical component was the FinancialFeatureEngineer transformer, which derives economic variables before the model sees the data. This calculates the Present Value (PV) of the loan using standard amortization formulas.\n# src/data_pipeline.py Snippet\n\ndef engineer_financial_features(df):\n    \"\"\"\n    Derives the Principal (PV) using the Annuity Formula.\n    PV = Installment * (1 - (1+r)^-n) / r\n    \"\"\"\n    monthly_rate = df['int.rate'] / 12\n    term = 36 # Assumed 36-month term\n    \n    # Vectorized Calculation of Risk Exposure\n    df['principal'] = df['installment'] * (\n        1 - (1 + monthly_rate)**(-term)\n    ) / monthly_rate\n    \n    return df\n\n\n\n\n3. Modeling Strategy\n\nModel Selection\nI benchmarked three architectures:\n\nLogistic Regression: Linear, highly interpretable (industry standard for compliance).\nRandom Forest: Handles non-linearities but prone to overfitting on noise.\nXGBoost: Gradient boosting for maximizing weak signals.\n\nWinner: Logistic Regression. Surprisingly, the simpler linear model outperformed the complex tree ensembles on the test set (AUC 0.685 vs 0.67 & 0.66 respectively). This suggests the signal in this specific dataset is largely linear (e.g., FICO scores linearly correlate with risk), and the complex models were overfitting the noise.\n\n\nThe “Strategy Curve”\nInstead of accepting the default decision threshold of 0.5, I implemented a custom evaluation function to plot Profit vs. Approval Rate.\n\nX-Axis: Percentage of loans approved (sorted by safety).\nY-Axis: Total Portfolio Profit.\n\n This visualization allows stakeholders to visually “pick” their risk appetite.\n\n\n\n\n4. Results & Analysis\n\nPerformance Metrics\nThe final model demonstrated robust separation power between “Good” and “Bad” loans.\n\nROC-AUC: 0.685\nPrecision (Class 0 - Paid): 0.86\nRecall (Class 1 - Default): Tunable via threshold.\n\n\n\nBusiness Insight: The Optimization Peak\nThe analysis revealed a convex profit curve.\n\nAt 100% Approval: The portfolio absorbs all toxic assets, significantly reducing net profit.\nAt 82.7% Approval (The Peak): By cutting the bottom 17.3% of risky applicants, we maximize profit at $1,264,043.\nBelow 80% Approval: We start rejecting too many good customers, and the opportunity cost (lost interest) begins to outweigh the savings from avoided defaults.\n\n\n\n\n\n\n\nManagerial Conclusion: The optimal policy is strict but not draconian. We should approve the safest ~83% of applicants. This creates a projected 40% increase in profitability compared to a naive strategy.\n\n\n\n\n\n\n\n5. Key Risk Drivers (Interpretation)\nUsing coefficient analysis (and SHAP values for tree benchmarks), the primary drivers of default were:\n\nFICO Score: The single strongest predictor. Lower scores exponentially increase default odds.\nInquiries (Last 6 Months): A high velocity of recent credit applications is a strong distress signal.\nInterest Rate: The market is efficient—loans priced with higher rates did default more often, confirming that the original underwriters correctly identified risk (though they often underpriced it).\n\n\n\n\nConclusion\nThis project demonstrates that Data Science is not just about prediction; it is about decision-making.\nBy translating a classification problem into a financial optimization problem, I delivered a model that speaks the language of the business: Profit, ROI, and Risk Exposure.\n\nCode Repository: View on GitHub\nTech Stack: Python, Scikit-Learn, Pandas, Matplotlib."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rafael Condé Gomes",
    "section": "",
    "text": "I am a double major in Econometrics and Information Science (concentration in Data Science & Analytics) at the University of South Florida (Class of 2026)\nI believe valid decisions require solid evidence, yet they are rarely based on it.\nMy focus is bridging that gap. I combine econometric rigor with data science to transform ambiguity into actionable insights. My goal is to replace intuition with inference, helping organizations optimize efficiency and solve complex real-world problems with precision.\n\n\n\nTechnology Director | Hexa Junior Consulting Company\nResearch Associate | USF Student Investment Club (Macroeconomics)\nTutor | University of South Florida Athletics\n\n\n\n\n\nCredit Risk Optimization: Developed a profit-maximizing threshold model for loan default prediction.\n\n\n\n\n\n\n\nPortfolio & Code: View my technical projects here."
  },
  {
    "objectID": "index.html#data-scientist-quantitative-analyst",
    "href": "index.html#data-scientist-quantitative-analyst",
    "title": "Rafael Condé Gomes",
    "section": "",
    "text": "I am a double major in Econometrics and Information Science (concentration in Data Science & Analytics) at the University of South Florida (Class of 2026)\nI believe valid decisions require solid evidence, yet they are rarely based on it.\nMy focus is bridging that gap. I combine econometric rigor with data science to transform ambiguity into actionable insights. My goal is to replace intuition with inference, helping organizations optimize efficiency and solve complex real-world problems with precision.\n\n\n\nTechnology Director | Hexa Junior Consulting Company\nResearch Associate | USF Student Investment Club (Macroeconomics)\nTutor | University of South Florida Athletics\n\n\n\n\n\nCredit Risk Optimization: Developed a profit-maximizing threshold model for loan default prediction.\n\n\n\n\n\n\n\nPortfolio & Code: View my technical projects here."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Technical Projects",
    "section": "",
    "text": "Stack: Python, Scikit-Learn, Pandas, Matplotlib, Seaborn\nThe Problem: Standard default prediction models maximize accuracy (AUC) but often fail to maximize profit. A bank loses more money on a False Negative (default) than it gains from a True Negative (repayment).\nThe Solution: I engineered a Threshold Optimization Engine that shifts the decision boundary based on the specific cost-benefit matrix of the loan portfolio.\n\nStrategy: Prioritized recall for high-risk segments to minimize costly defaults.\nResult: While raw accuracy remained comparable to baseline, the risk-adjusted revenue improved significantly by rejecting “borderline” applicants that standard models would have accepted.\n\nView Full Analysis & Code →\n\n\n\n\nStack: R (ggplot2, dplyr), Geospatial Analysis\nThe Problem: Ride-sharing platforms face a “Cold Start” problem every day: Where should drivers position themselves before demand spikes? Inefficient positioning leads to higher wait times and lost revenue.\nThe Solution: I conducted a spatiotemporal analysis of 4.5M+ Uber pickups to identify high-density clusters and temporal demand cycles.\n\nInsight: Identified a non-linear demand surge on Thursdays (not Fridays) suggesting commuter-heavy usage over nightlife.\nApplication: These heatmaps serve as a proxy for “Pre-Surge” indicators for fleet management.\n\nView Visual Analysis →"
  },
  {
    "objectID": "projects.html#featured-case-studies",
    "href": "projects.html#featured-case-studies",
    "title": "Technical Projects",
    "section": "",
    "text": "Stack: Python, Scikit-Learn, Pandas, Matplotlib, Seaborn\nThe Problem: Standard default prediction models maximize accuracy (AUC) but often fail to maximize profit. A bank loses more money on a False Negative (default) than it gains from a True Negative (repayment).\nThe Solution: I engineered a Threshold Optimization Engine that shifts the decision boundary based on the specific cost-benefit matrix of the loan portfolio.\n\nStrategy: Prioritized recall for high-risk segments to minimize costly defaults.\nResult: While raw accuracy remained comparable to baseline, the risk-adjusted revenue improved significantly by rejecting “borderline” applicants that standard models would have accepted.\n\nView Full Analysis & Code →\n\n\n\n\nStack: R (ggplot2, dplyr), Geospatial Analysis\nThe Problem: Ride-sharing platforms face a “Cold Start” problem every day: Where should drivers position themselves before demand spikes? Inefficient positioning leads to higher wait times and lost revenue.\nThe Solution: I conducted a spatiotemporal analysis of 4.5M+ Uber pickups to identify high-density clusters and temporal demand cycles.\n\nInsight: Identified a non-linear demand surge on Thursdays (not Fridays) suggesting commuter-heavy usage over nightlife.\nApplication: These heatmaps serve as a proxy for “Pre-Surge” indicators for fleet management.\n\nView Visual Analysis →"
  }
]